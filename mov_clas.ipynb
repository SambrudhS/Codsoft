{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#new book\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "9FCdqNoiQB5d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GMYtNNhiQHXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv(\"/content/drive/MyDrive/Genre Classification Dataset/train_data.txt\",sep=\":::\",names=[\n",
        "    'slno','title','genre','plot'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce81b404-6f68-4a89-a7f6-48b1e570f0b2",
        "id": "BIZY911kQPys"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-3c25b41a4320>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  df= pd.read_csv(\"/content/drive/MyDrive/Genre Classification Dataset/train_data.txt\",sep=\":::\",names=[\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6wT2xLJQVCt",
        "outputId": "cba7c32f-ed72-454b-9dee-00bb18caf20f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize, remove stopwords, and lemmatize\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.isalnum()]\n",
        "    return ' '.join(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7bGc9J2QWiS",
        "outputId": "3739827f-3f7f-47ac-dfb3-58648e568709"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "df['processed_plot'] = df['plot'].apply(preprocess_text)\n",
        "\n",
        "# Preprocess the genres\n",
        "df['genre'] = df['genre'].apply(lambda x: x.split('|'))\n",
        "mlb = MultiLabelBinarizer()\n",
        "genres_encoded = mlb.fit_transform(df['genre'])"
      ],
      "metadata": {
        "id": "CcQvP9IXQko1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f0cb3bb-08ea-4e32-c601-3f6620a36d0d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genres_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8Do75YsQ337",
        "outputId": "4300c342-4aa4-42c3-c8b5-976c44f30cc5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 1, 0, 0],\n",
              "       [0, 1, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget http://nlp.stanford.edu/df/glove.6B.zip\n",
        "#!unzip glove*.zip\n",
        "# Load the pre-trained GloVe embeddings\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/MyDrive/Genre Classification Dataset/glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# Tokenize and pad the text sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['processed_plot'])\n",
        "sequences = tokenizer.texts_to_sequences(df['processed_plot'])\n",
        "word_index = tokenizer.word_index\n",
        "padded_sequences = pad_sequences(sequences, maxlen=300)\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "hwwCwcH5Rxhc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=300, trainable=False))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(len(mlb.classes_), activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, genres_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=64, epochs=10, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "Vh2FcTg5TDmh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b4dba0-c190-4249-a313-311b000a7e69"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "678/678 [==============================] - 174s 253ms/step - loss: 0.1181 - accuracy: 0.3921 - val_loss: 0.1050 - val_accuracy: 0.4457\n",
            "Epoch 2/10\n",
            "678/678 [==============================] - 170s 250ms/step - loss: 0.1007 - accuracy: 0.4555 - val_loss: 0.0960 - val_accuracy: 0.4746\n",
            "Epoch 3/10\n",
            "678/678 [==============================] - 169s 250ms/step - loss: 0.0947 - accuracy: 0.4851 - val_loss: 0.0914 - val_accuracy: 0.5002\n",
            "Epoch 4/10\n",
            "678/678 [==============================] - 169s 249ms/step - loss: 0.0905 - accuracy: 0.5044 - val_loss: 0.0889 - val_accuracy: 0.5134\n",
            "Epoch 5/10\n",
            "678/678 [==============================] - 169s 250ms/step - loss: 0.0873 - accuracy: 0.5248 - val_loss: 0.0855 - val_accuracy: 0.5337\n",
            "Epoch 6/10\n",
            "678/678 [==============================] - 169s 249ms/step - loss: 0.0843 - accuracy: 0.5404 - val_loss: 0.0835 - val_accuracy: 0.5413\n",
            "Epoch 7/10\n",
            "678/678 [==============================] - 169s 249ms/step - loss: 0.0822 - accuracy: 0.5502 - val_loss: 0.0821 - val_accuracy: 0.5517\n",
            "Epoch 8/10\n",
            "678/678 [==============================] - 169s 249ms/step - loss: 0.0803 - accuracy: 0.5610 - val_loss: 0.0802 - val_accuracy: 0.5532\n",
            "Epoch 9/10\n",
            "678/678 [==============================] - 169s 249ms/step - loss: 0.0789 - accuracy: 0.5693 - val_loss: 0.0802 - val_accuracy: 0.5561\n",
            "Epoch 10/10\n",
            "678/678 [==============================] - 169s 249ms/step - loss: 0.0776 - accuracy: 0.5750 - val_loss: 0.0787 - val_accuracy: 0.5655\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7aba0c1ca290>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Genre Classification Dataset/lstm4.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ],
      "metadata": {
        "id": "uxbweFdCXhlD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Genre Classification Dataset/lstm3.pkl', 'rb') as f:\n",
        "  model=pickle.load(f)"
      ],
      "metadata": {
        "id": "mPVW5pC_el_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "precision = precision_score(y_test, y_pred_classes, average='micro')\n",
        "recall = recall_score(y_test, y_pred_classes, average='micro')\n",
        "f1 = f1_score(y_test, y_pred_classes, average='micro')\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1-score: {f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9cqUfdNX3fE",
        "outputId": "9bf7afd6-0784-4c2f-d039-3ba1c25aa53d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "339/339 [==============================] - 13s 37ms/step\n",
            "Accuracy: 0.4260813428018076\n",
            "Precision: 0.6888034314450525\n",
            "Recall: 0.42949368256017706\n",
            "F1-score: 0.5290842990229493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify misclassified instances\n",
        "misclassified = np.where(np.any(y_test != y_pred_classes, axis=1))[0]\n",
        "\n",
        "# Analyze misclassified instances and suggest improvements\n",
        "for idx in misclassified[:50]:\n",
        "    print(f\"Movie: {df['title'][idx]}\")\n",
        "    print(f\"Actual genres: {', '.join(df['genre'][idx])}\")\n",
        "    print(f\"Predicted genres: {', '.join(mlb.classes_[y_pred_classes[idx] == 1])}\")\n",
        "    print()\n",
        "print(len(y_pred_classes),len(misclassified))"
      ],
      "metadata": {
        "id": "GNCmlDy7YXNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}